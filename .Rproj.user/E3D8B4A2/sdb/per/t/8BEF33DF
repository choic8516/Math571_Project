{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Math571_project\"\noutput: html_document\n---\n\n# Getting NYT headlines version 1\n```{r}\nlibrary(jsonlite)\nlibrary(magrittr)\nlibrary(tidyverse)\nlibrary(lubridate)\n\nConnor_key <- \"5b15a6f528e7486cbd51dd2f47265d25\" # Please use your own API key. you can get it from nyt api website. \n\nbegin_date <- ymd(\"19891231\") # set your beginning date\nend_date <- ymd(\"19900101\") # set you ending that which is only one day apart.\ntemp_begin_date <- begin_date\ntemp_end_date <- end_date\nselected_end_date <- ymd(\"19900201\") # set the final end date for your search.\n\n# search limited to \"Financial\" articles\n# This should result in higher correlation to stock price changes than having articles from sports and politics, etc. \n# It was hard for me to random sample from too many number of pages. It will take forever. \n# I ran this code for just one month but it still took awhile(1 min?). It will depend on you laptop performance but we can divide our work and it will be fine. \n# If you can improve my code plz do and let us know of any improvements.  \nwhile (temp_end_date <= selected_end_date) {\nfor (page in c(0:2)) {\n        art <- fromJSON(paste0(\"http://api.nytimes.com/svc/search/v2/articlesearch.json?begin_date=\",temp_begin_date,\"&end_date=\",temp_end_date,\"&api-key=\",Connor_key,\"&page=\",page,\"&fq=news_desk:(Financial)\"), flatten = TRUE) %>%  data.frame()\n        if (page == 0) {\n                news_head <- c(as.character(temp_end_date),art$response.docs.headline.main)\n        } else {\n        news_head <- c(news_head,art$response.docs.headline.main)\n        }\n        Sys.sleep(1)\n}\nif (temp_begin_date == begin_date) {               \n        headlines <- data.frame(t(news_head),stringsAsFactors = F)\n} else {\n        headlines <- rbind(headlines,data.frame(t(news_head)))\n}\ntemp_begin_date <- temp_begin_date + 1\ntemp_end_date <-  temp_end_date + 1\n}\n\nView(headlines)\n\n#returns excel file with headlines for selected dates above. \n#we can later combine all our excels files into one. \nWriteXLS::WriteXLS(headlines,\"nyt_headlines.xls\")\n```\n\n# Getting NYT headlines version 2\n```{r}\nlibrary(jsonlite)\nlibrary(magrittr)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(xlsx)\n\nConnor_key <- \"5b15a6f528e7486cbd51dd2f47265d25\" #use your key!\n\nyear <- 2000 #set year\nstart_month <- 1 #starting month\nend_month <- 2 #ending month\ncount <- 0 # cnt variable use within for loop\nnum_samples <- 25 # set number of sample of articles to choose from all categories of new articels(i.e. sports, finance, business, etc.)\n\n# In this for-loop, we get headlines from each month of particular year and merge those monthly datasets. Final product is excel file with headlines and dates belonging to those months. \nfor (month in c(start_month:end_month)) {\nurl.nyt.archive<-paste0(\"https://api.nytimes.com/svc/archive/v1/\",year,\"/\",month,\".json\",\"?api-key=\",Connor_key)\ndf.json<-fromJSON(url.nyt.archive, flatten = T)\n\nif (count == 0) {\n        temp <- df.json$response$docs[c('type_of_material','headline.main',\"pub_date\")]\n        temp <- temp %>%\n                filter(type_of_material == \"News\")\n        temp <- temp %>%\n                group_by(pub_date) %>%\n                nest()\n        \n        temp_1 <- data.frame(headline = t(sample(temp$data[[1]]$headline.main,size = num_samples)), stringsAsFactors = F)\n\n        for (i in c(2:nrow(temp))) {\n        temp_1 <- rbind(temp_1, data.frame(headline = t(sample(temp$data[[i]]$headline.main,size = num_samples)), stringsAsFactors = F))        \n}\n\n        temp <- cbind(temp,temp_1)\n        \n}\nelse {\ntemp_df <- df.json$response$docs[c('type_of_material','headline.main',\"pub_date\")]\ntemp_df <- temp_df %>%\n                filter(type_of_material == \"News\")\ntemp_df <- temp_df %>%\n                group_by(pub_date) %>%\n                nest()\n\n\n\ntemp_1 <- data.frame(headline = t(sample(temp_df$data[[1]]$headline.main,size = num_samples)), stringsAsFactors = F)\n\nfor (i in c(2:nrow(temp_df))) {\n        temp_1 <- rbind(temp_1, data.frame(headline = t(sample(temp_df$data[[i]]$headline.main,size = num_samples)), stringsAsFactors = F))        \n}\n\ntemp_df <- cbind(temp_df,temp_1)\n\ntemp <- rbind(temp,temp_df)\n}\ncount <- count + 1\n}\n\n\nView(temp)\n\nwrite.xlsx(temp[,-2], \"nyt_headlines.xls\")\n```\n\n\n# Converting headlines into scores (using Loughran and McDonald lexicon)\n```{r}\nlibrary(xlsx)\nlibrary(tidytext)\nlibrary(tidyverse)\ndata <- read.xlsx(\"nyt_headlines.xls\", sheetIndex = 1,stringsAsFactors=FALSE)\n#subset of the data above.\ntest_data <- data[1:2,2:ncol(data)]\n\n# this score chart is arbitrary! We can choose other numbers\ntemp_score_chart <- data.frame(sentiment = c(\"negative\", \"positive\", \"uncertainty\", \"litigious\", \"constraining\", \"superfluous\"), scores = c(-1,1,-1,-1,-1,0), stringsAsFactors = F)\n\n# for this sentimental analysis I used Loughran and McDonald lexicon. Their lexicon is written specifically for finance. However, we have a lot of words from headlines that are missing in their lexicon. Lexicon specialized for finance is not easy to find. \n\n#function to convert words to scores\nto_scores <- function(df) {\n        val <- df %>%\n                as.data.frame(stringsAsFactors = F) %>%\n                unnest_tokens(word,\".\") %>%\n                inner_join(get_sentiments(\"loughran\"), by = \"word\") %>%\n                inner_join(temp_score_chart, by = \"sentiment\") %>%\n                summarise(sum(scores))\n        unlist(val)\n}\n\n#this code may be able to be written more efficiently. \nfor (i in c(1:nrow(test_data))) {\n        for (j in c(1:ncol(test_data))) {\n                test_data[i,j] = to_scores(test_data[i,j])\n        }\n}\n\n#result of conversion to scores\ntest_data\n```\n\n# Converting headlines into scores (using SentimentAnalysis package)\n```{r}\nlibrary(xlsx)\nlibrary(tidyverse)\nlibrary(SentimentAnalysis)\n\ndata_1 <- read.xlsx(\"nyt_headlines.xls\", sheetIndex = 1,stringsAsFactors=FALSE)\nsubset_of_data_1 <- data_1[1:2,-1]\n\nconvert_to_score <- function(df) {\n        sentiment <- analyzeSentiment(df)\n        sentiment$SentimentHE # I chose here Harvard General Inquirer lexicon but you can also choose other lexicons. There are I think 3 other ones including Loughran and McDonald lexicon\n}\n\n# this following operation takes about 20 seconds for my laptop. I feel like it takes little too long. \nsubset_of_data_1[] <- lapply(subset_of_data_1, convert_to_score)\nsubset_of_data_1\n```\n\n\n",
    "created" : 1522164496992.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2635789854",
    "id" : "8BEF33DF",
    "lastKnownWriteTime" : 1521947321,
    "last_content_update" : 1522164503564,
    "path" : "C:/OneDrive/2018/_MATH571/Math571_Project/math571_project.Rmd",
    "project_path" : "math571_project.Rmd",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}