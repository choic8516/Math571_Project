---
title: "kNN"
author: "Jingwei Li"
date: "April 19, 2018"
output: html_document
---

```{r setup, cache=TRUE}
knitr::opts_chunk$set(echo = TRUE)
```

## Kaggle data
### Load cleaned kaggle data
```{r}
library(xlsx)
file <- file.path('../Connor/Complete Data/kaggle_headlines_scored_classified.xlsx')
k_data <- read.xlsx(file = file, sheetIndex = 1, stringsAsFactors = F)
k_data$Date <- as.Date(k_data$Date)
k_data$Label <- as.factor(k_data$Label)
```

### Divide Train and Test
Pick Jan 1, 2016 as the borderline of two sets.
```{r}
set.seed(999)
k_train <- k_data[k_data$Date < as.Date("2016-01-01"),]
k_test <- k_data[k_data$Date >= as.Date("2016-01-01"),]
stopifnot((nrow(k_train) + nrow(k_test)) == nrow(k_data))
```

### kNN
Remove target varibale and date from train set
```{r}
library(class)
library(caret)
targetVars <- c("Label")
xVars <- c("Very.Negative", "Negative", "Neutral", "Postive", "Very.Positive")
k_train_label <- k_train[, targetVars]
k_test_label <- k_test[, targetVars]
k_train_xVars <- k_train[, xVars]
k_test_xVars <- k_test[, xVars]

knn_function <- function(k_value){
        temp_knn <- knn(train = k_train_xVars, test = k_test_xVars, cl = k_train_label, k = k_value)
        temp_knn_cm <- confusionMatrix(reference = k_test_label, data = temp_knn)
        return(temp_knn_cm$overall[1])
}

k_range <- seq(1,300)
result_list <- lapply(k_range, knn_function)
plot(k_range, result_list, type = "l")
opt_k <- which.max(result_list)
opt_acc <- knn_function(opt_k)
```
It looks we more than one picking areas for k. The optimal k-value is `r opt_k`, at that point we have accuracy about `r round(opt_acc, digits = 4)*100`%.

### 3rd Day Prediction w/ kaggle
```{r}
k_3rd_train_xVars <- cbind(k_train[1:(nrow(k_train)-2), xVars],
                           k_train[2:(nrow(k_train)-1), xVars],
                           k_train[3:nrow(k_train), xVars])
k_3rd_train_label <- k_train[3:nrow(k_train), targetVars]

k_3rd_test_xVars <- cbind(k_test[1:(nrow(k_test)-2), xVars],
                           k_test[2:(nrow(k_test)-1), xVars],
                           k_test[3:nrow(k_test), xVars])
k_3rd_test_label <- k_test[3:nrow(k_test), targetVars]

knn_function_3rd <- function(k_value){
        temp_knn <- knn(train = k_3rd_train_xVars, test = k_3rd_test_xVars, cl = k_3rd_train_label, k = k_value)
        temp_knn_cm <- confusionMatrix(reference = k_3rd_test_label, data = temp_knn)
        return(temp_knn_cm$overall[1])
}

# k_range <- seq(1,300)
result_list <- lapply(k_range, knn_function_3rd)
plot(k_range, result_list, type = "l")
opt_k <- which.max(result_list)
opt_acc <- knn_function(opt_k)
```
The optimal k-value is `r opt_k`, at that point we have accuracy about `r round(opt_acc, digits = 4)*100`%.

## NYT data
### Load cleaned kaggle data
NA is removed for kNN model. NA data shows up on weekends.
```{r}
library(data.table)
file <- file.path('../Connor/Complete Data/classified_scores_&_stock_movement_1.csv')
nyt_data <- as.data.frame(fread(file = file))
nyt_data$Date <- as.Date(nyt_data$Date)
nyt_data$Direction <- as.factor(nyt_data$Direction)
nyt_data_rm <- na.omit(nyt_data)
```

### Divide Train and Test
Pick Jan 1, 2014 as the borderline of two sets.
```{r}
nyt_train <- nyt_data_rm[nyt_data_rm$Date < as.Date("2014-01-01"),]
nyt_test <- nyt_data_rm[nyt_data_rm$Date >= as.Date("2014-01-01"),]
stopifnot((nrow(nyt_train) + nrow(nyt_test)) == nrow(nyt_data_rm))
```

### kNN
```{r}
library(class)
library(caret)
targetVars <- c("Direction")
# xVars <- c("Very.Negative", "Negative", "Neutral", "Postive", "Very.Positive")
nyt_train_label <- nyt_train[, targetVars]
nyt_test_label <- nyt_test[, targetVars]
nyt_train_xVars <- nyt_train[, xVars]
nyt_test_xVars <- nyt_test[, xVars]

knn_function2 <- function(k_value){
        temp_knn <- knn(train = nyt_train_xVars, test = nyt_test_xVars, cl = nyt_train_label, k = k_value)
        temp_knn_cm <- confusionMatrix(reference = nyt_test_label, data = temp_knn)
        return(temp_knn_cm$overall[1])
}

k_range <- seq(1,300)
result_list <- lapply(k_range, knn_function2)
plot(k_range, result_list, type = "l")
opt_k <- which.max(result_list)
opt_acc <- knn_function2(opt_k)
```
The optimal k-value is `r opt_k`, at that point we have accuracy about `r round(opt_acc, digits = 4)*100`%.

### 3rd Day Prediction w/ NYT
```{r}
nyt_3rd_train_xVars <- cbind(nyt_train[1:(nrow(nyt_train)-2), xVars],
                           nyt_train[2:(nrow(nyt_train)-1), xVars],
                           nyt_train[3:nrow(nyt_train), xVars])
nyt_3rd_train_label <- nyt_train[3:nrow(nyt_train), targetVars]

nyt_3rd_test_xVars <- cbind(nyt_test[1:(nrow(nyt_test)-2), xVars],
                           nyt_test[2:(nrow(nyt_test)-1), xVars],
                           nyt_test[3:nrow(nyt_test), xVars])
nyt_3rd_test_label <- nyt_test[3:nrow(nyt_test), targetVars]

knn_function2_3rd <- function(k_value){
        temp_knn <- knn(train = nyt_3rd_train_xVars, test = nyt_3rd_test_xVars, cl = nyt_3rd_train_label, k = k_value)
        temp_knn_cm <- confusionMatrix(reference = nyt_3rd_test_label, data = temp_knn)
        return(temp_knn_cm$overall[1])
}

# k_range <- seq(1,300)
result_list <- lapply(k_range, knn_function2_3rd)
plot(k_range, result_list, type = "l")
opt_k <- which.max(result_list)
opt_acc <- knn_function(opt_k)
```
The optimal k-value is `r opt_k`, at that point we have accuracy about `r round(opt_acc, digits = 4)*100`%.