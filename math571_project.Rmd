---
title: "Math571_project"
output: html_document
---

# Getting NYT headlines
```{r}
library(jsonlite)
library(magrittr)
library(tidyverse)
library(lubridate)

Connor_key <- "5b15a6f528e7486cbd51dd2f47265d25" # Please use your own API key. you can get it from nyt api website. 

begin_date <- ymd("19891231") # set your beginning date
end_date <- ymd("19900101") # set you ending that which is only one day apart.
temp_begin_date <- begin_date
temp_end_date <- end_date
selected_end_date <- ymd("19900201") # set the final end date for your search.

# search limited to "Financial" articles
# This should result in higher correlation to stock price changes than having articles from sports and politics, etc. 
# It was hard for me to random sample from too many number of pages. It will take forever. 
# I ran this code for just one month but it still took awhile(1 min?). It will depend on you laptop performance but we can divide our work and it will be fine. 
# If you can improve my code plz do and let us know of any improvements.  
while (temp_end_date <= selected_end_date) {
for (page in c(0:2)) {
        art <- fromJSON(paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?begin_date=",temp_begin_date,"&end_date=",temp_end_date,"&api-key=",Connor_key,"&page=",page,"&fq=news_desk:(Financial)"), flatten = TRUE) %>%  data.frame()
        if (page == 0) {
                news_head <- c(as.character(temp_end_date),art$response.docs.headline.main)
        } else {
        news_head <- c(news_head,art$response.docs.headline.main)
        }
        Sys.sleep(1)
}
if (temp_begin_date == begin_date) {               
        headlines <- data.frame(t(news_head),stringsAsFactors = F)
} else {
        headlines <- rbind(headlines,data.frame(t(news_head)))
}
temp_begin_date <- temp_begin_date + 1
temp_end_date <-  temp_end_date + 1
}

View(headlines)

#returns excel file with headlines for selected dates above. 
#we can later combine all our excels files into one. 
WriteXLS::WriteXLS(headlines,"nyt_headlines.xls")
```

# Converting headlines into scores
```{r}
library(xlsx)
library(tidytext)
library(tidyverse)
data <- read.xlsx("nyt_headlines.xls", sheetIndex = 1,stringsAsFactors=FALSE)
#subset of the data above.
test_data <- data[1:2,2:ncol(data)]

# this score chart is arbitrary! We can choose other numbers
temp_score_chart <- data.frame(sentiment = c("negative", "positive", "uncertainty", "litigious", "constraining", "superfluous"), scores = c(-1,1,-1,-1,-1,0), stringsAsFactors = F)

# for this sentimental analysis I used Loughran and McDonald lexicon. Their lexicon is written specifically for finance. However, we have a lot of words from headlines that are missing in their lexicon. Lexicon specialized for finance is not easy to find. 

#function to convert words to scores
to_scores <- function(df) {
        val <- df %>%
                as.data.frame(stringsAsFactors = F) %>%
                unnest_tokens(word,".") %>%
                inner_join(get_sentiments("loughran"), by = "word") %>%
                inner_join(temp_score_chart, by = "sentiment") %>%
                summarise(sum(scores))
        unlist(val)
}

#this code may be able to be written more efficiently. 
for (i in c(1:nrow(test_data))) {
        for (j in c(1:ncol(test_data))) {
                test_data[i,j] = to_scores(test_data[i,j])
        }
}

#result of conversion to scores
test_data
```



