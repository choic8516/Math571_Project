---
title: "Math571_project"
output: html_document
---

# Getting NYT headlines
```{r}
library(jsonlite)
library(magrittr)
library(tidyverse)
library(lubridate)
library(httr)

cristobal_key <- "5b4e1b897deb448caa65b58d6c5941ab" # Please use your own API key. you can get it from nyt api website. 




begin_date <- ymd("19891231") # set your beginning date
end_date <- ymd("19900101") # set you ending that which is only one day apart.
temp_begin_date <- begin_date
temp_end_date <- end_date
selected_end_date <- ymd("19900201") # set the final end date for your search.

# search limited to "Financial" articles
page<-1
urlGet<-paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?begin_date=",temp_begin_date,"&end_date=",temp_end_date,"&api-key=",cristobal_key,"&page=",page,"&fq=news_desk:(Financial)")

json_file<-GET(urlGet)
               
art <- fromJSON(json_file, flatten = TRUE)

ls.index<-sapply(art$response$docs,FUN = class)=="list"
thing<-art$response$docs[!ls.index]
View(thing)
type$copyright
# If you can improve my code plz do and let us know of any improvements.  
while (temp_end_date <= selected_end_date) {
for (page in c(0:2)) {
        art <- fromJSON(paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?begin_date=",temp_begin_date,"&end_date=",temp_end_date,"&api-key=",Connor_key,"&page=",page,"&fq=news_desk:(Financial)"), flatten = TRUE) %>%  data.frame()
        if (page == 0) {
                news_head <- c(as.character(temp_end_date),art$response.docs.headline.main)
        } else {
        news_head <- c(news_head,art$response.docs.headline.main)
        }
        Sys.sleep(1)
}
if (temp_begin_date == begin_date) {               
        headlines <- data.frame(t(news_head),stringsAsFactors = F)
} else {
        headlines <- rbind(headlines,data.frame(t(news_head)))
}
temp_begin_date <- temp_begin_date + 1
temp_end_date <-  temp_end_date + 1
}

View(headlines)

#returns excel file with headlines for selected dates above. 
#we can later combine all our excels files into one. 
WriteXLS::WriteXLS(headlines,"nyt_headlines.xls")
```

# Converting headlines into scores (using Loughran and McDonald lexicon)
```{r}
library(xlsx)
library(tidytext)
library(tidyverse)
data <- read.xlsx("nyt_headlines.xls", sheetIndex = 1,stringsAsFactors=FALSE)
#subset of the data above.
test_data <- data[1:2,2:ncol(data)]

# this score chart is arbitrary! We can choose other numbers
temp_score_chart <- data.frame(sentiment = c("negative", "positive", "uncertainty", "litigious", "constraining", "superfluous"), scores = c(-1,1,-1,-1,-1,0), stringsAsFactors = F)

# for this sentimental analysis I used Loughran and McDonald lexicon. Their lexicon is written specifically for finance. However, we have a lot of words from headlines that are missing in their lexicon. Lexicon specialized for finance is not easy to find. 

#function to convert words to scores
to_scores <- function(df) {
        val <- df %>%
                as.data.frame(stringsAsFactors = F) %>%
                unnest_tokens(word,".") %>%
                inner_join(get_sentiments("loughran"), by = "word") %>%
                inner_join(temp_score_chart, by = "sentiment") %>%
                summarise(sum(scores))
        unlist(val)
}

#this code may be able to be written more efficiently. 
for (i in c(1:nrow(test_data))) {
        for (j in c(1:ncol(test_data))) {
                test_data[i,j] = to_scores(test_data[i,j])
        }
}

#result of conversion to scores
test_data
```

# Converting headlines into scores (using SentimentAnalysis package)
```{r}
library(xlsx)
library(tidyverse)
library(SentimentAnalysis)

data_1 <- read.xlsx("nyt_headlines.xls", sheetIndex = 1,stringsAsFactors=FALSE)
subset_of_data_1 <- data_1[1:2,-1]

convert_to_score <- function(df) {
        sentiment <- analyzeSentiment(df)
        sentiment$SentimentHE # I chose here Harvard General Inquirer lexicon but you can also choose other lexicons. There are I think 3 other ones including Loughran and McDonald lexicon
}

# this following operation takes about 20 seconds for my laptop. I feel like it takes little too long. 
subset_of_data_1[] <- lapply(subset_of_data_1, convert_to_score)
subset_of_data_1
```


